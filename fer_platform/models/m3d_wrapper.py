"""
M3Dæ¨¡å‹æ¨ç†åŒ…è£…å™¨
æä¾›ç»Ÿä¸€çš„M3Dæ¨¡å‹æ¨ç†æ¥å£
"""

import torch
import torch.nn.functional as F
import numpy as np
import cv2
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

from core_model.models.M3D import M3DFEL
from fer_platform.config import config

class M3DInferenceWrapper:
    """M3Dæ¨¡å‹æ¨ç†åŒ…è£…å™¨"""
    
    # è¡¨æƒ…ç±»åˆ«æ˜ å°„
    EMOTION_LABELS = {
        0: "æ„¤æ€’",
        1: "åŒæ¶", 
        2: "ææƒ§",
        3: "å¼€å¿ƒ",
        4: "ä¸­æ€§",
        5: "æ‚²ä¼¤",
        6: "æƒŠè®¶"
    }
    
    def __init__(self, model_path: str = None, device: str = "auto"):
        """
        åˆå§‹åŒ–M3Dæ¨ç†åŒ…è£…å™¨
        
        Args:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
            device: è®¾å¤‡ç±»å‹ ('auto', 'cpu', 'cuda')
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®šï¼‰
        if model_path is None:
            model_path = config.get_model_path()
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
            
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        self.model_args = self._get_default_args()
        
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = M3DFEL(self.model_args)
            self.model.to(self.device)
            self.model.eval()
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            if model_path and Path(model_path).exists():
                self.load_model(model_path)
                self.logger.info(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
            else:
                self._handle_no_pretrained_model(model_path)
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _handle_no_pretrained_model(self, model_path: str = None):
        """å¤„ç†æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„æƒ…å†µ"""
        if model_path:
            self.logger.warning(f"âŒ æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        model_info = config.get_model_info()
        
        self.logger.info("="*60)
        self.logger.info("ğŸ” é¢„è®­ç»ƒæ¨¡å‹çŠ¶æ€æ£€æŸ¥")
        self.logger.info("="*60)
        self.logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_info['model_dir']}")
        self.logger.info(f"ğŸ” æ”¯æŒçš„æ¨¡å‹æ–‡ä»¶: {', '.join(model_info['supported_files'])}")
        self.logger.info("ğŸ’¡ å¦‚éœ€æ·»åŠ é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹æ–‡ä»¶æ”¾åœ¨ä¸Šè¿°ç›®å½•ä¸­")
        self.logger.info("âš ï¸  å½“å‰ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼Œè¯†åˆ«ç²¾åº¦è¾ƒä½")
        self.logger.info("="*60)
        
        print("ğŸ“ æç¤º: å½“å‰ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿è¡Œï¼Œå¦‚éœ€æ›´å¥½çš„è¯†åˆ«æ•ˆæœï¼Œè¯·æ·»åŠ é¢„è®­ç»ƒæ¨¡å‹")
    
    def _get_default_args(self):
        """è·å–é»˜è®¤æ¨¡å‹å‚æ•°"""
        class Args:
            def __init__(self):
                self.num_classes = config.MODEL_CONFIG['num_classes']
                self.num_frames = config.MODEL_CONFIG['num_frames']
                self.instance_length = 4
                self.crop_size = config.MODEL_CONFIG['crop_size']
                self.gpu_ids = [0] if torch.cuda.is_available() else []
        
        return Args()
    
    def load_model(self, model_path: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
                
            self.model.load_state_dict(state_dict, strict=False)
            self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def preprocess_frames(self, frames: List[np.ndarray]) -> torch.Tensor:
        """
        é¢„å¤„ç†è§†é¢‘å¸§
        
        Args:
            frames: åŸå§‹è§†é¢‘å¸§åˆ—è¡¨
            
        Returns:
            preprocessed_tensor: é¢„å¤„ç†åçš„å¼ é‡ [B, T, C, H, W]
        """
        try:
            # ç¡®ä¿å¸§æ•°ä¸º16
            if len(frames) != 16:
                frames = self._sample_frames(frames, 16)
            
            processed_frames = []
            for frame in frames:
                # ç¡®ä¿frameæ˜¯numpyæ•°ç»„
                if not isinstance(frame, np.ndarray):
                    continue
                    
                # è°ƒæ•´å¤§å°åˆ°112x112
                frame = cv2.resize(frame, (112, 112))
                
                # BGRè½¬RGB
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                elif len(frame.shape) == 2:
                    # ç°åº¦å›¾è½¬RGB
                    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
                
                # å½’ä¸€åŒ–åˆ°[0,1]
                frame = frame.astype(np.float32) / 255.0
                
                # è½¬æ¢ä¸ºCHWæ ¼å¼
                frame = np.transpose(frame, (2, 0, 1))
                processed_frames.append(frame)
            
            # ç¡®ä¿æœ‰16å¸§
            while len(processed_frames) < 16:
                if processed_frames:
                    processed_frames.append(processed_frames[-1])
                else:
                    # åˆ›å»ºç©ºç™½å¸§
                    blank_frame = np.zeros((3, 112, 112), dtype=np.float32)
                    processed_frames.append(blank_frame)
            
            # å †å ä¸ºtensor: [T, C, H, W]
            tensor = torch.tensor(np.stack(processed_frames), dtype=torch.float32)
            
            # æ·»åŠ batchç»´åº¦å¹¶è°ƒæ•´ä¸ºæ­£ç¡®æ ¼å¼: [B, T, C, H, W] -> [1, 16, 3, 112, 112]
            tensor = tensor.unsqueeze(0)
            
            # ç¡®ä¿tensoræ˜¯è¿ç»­çš„ï¼Œé¿å…strideé—®é¢˜
            tensor = tensor.contiguous()
            
            return tensor.to(self.device)
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†å¸§æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤tensor
            return torch.zeros(1, 16, 3, 112, 112, dtype=torch.float32, device=self.device)
    
    def _sample_frames(self, frames: List[np.ndarray], target_num: int) -> List[np.ndarray]:
        """å‡åŒ€é‡‡æ ·è§†é¢‘å¸§"""
        if len(frames) <= target_num:
            # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€å¸§
            return frames + [frames[-1]] * (target_num - len(frames))
        
        # å‡åŒ€é‡‡æ ·
        indices = np.linspace(0, len(frames) - 1, target_num, dtype=int)
        return [frames[i] for i in indices]
    
    @torch.no_grad()
    def predict(self, frames: List[np.ndarray]) -> Dict[str, Any]:
        """
        é¢„æµ‹è¡¨æƒ…
        
        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            
        Returns:
            prediction_result: é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            # é¢„å¤„ç†
            input_tensor = self.preprocess_frames(frames)
            self.logger.debug(f"è¾“å…¥tensorå½¢çŠ¶: {input_tensor.shape}")
            
            # æ¨ç†
            output, features = self.model(input_tensor)
            self.logger.debug(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
            # ç¡®ä¿è¾“å‡ºæ˜¯2ç»´çš„ [batch, num_classes]
            if output.dim() > 2:
                # å¦‚æœè¾“å‡ºç»´åº¦å¤§äº2ï¼Œè¿›è¡Œreshape
                output = output.view(output.size(0), -1)
                if output.size(1) != self.model_args.num_classes:
                    # å¦‚æœä¸æ˜¯é¢„æœŸçš„ç±»åˆ«æ•°ï¼Œå–æœ€åå‡ ç»´
                    output = output[:, -self.model_args.num_classes:]
            elif output.dim() == 1:
                output = output.unsqueeze(0)
            
            # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            if output.size(1) != self.model_args.num_classes:
                self.logger.warning(f"è¾“å‡ºç»´åº¦å¼‚å¸¸: {output.shape}, æœŸæœ›: {self.model_args.num_classes}")
                # åˆ›å»ºé»˜è®¤è¾“å‡º
                output = torch.zeros(1, self.model_args.num_classes, device=self.device)
                output[0, 2] = 1.0  # é»˜è®¤ä¸ºä¸­æ€§è¡¨æƒ…
            
            # è®¡ç®—æ¦‚ç‡
            probabilities = F.softmax(output, dim=-1)
            predicted_class = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_class].item()
            
            # æ„å»ºç»“æœï¼ˆæ³¨æ„ï¼šå»é™¤numpyæ•°ç»„ï¼Œé¿å…JSONåºåˆ—åŒ–é—®é¢˜ï¼‰
            result = {
                'predicted_class': predicted_class,
                'predicted_label': self.EMOTION_LABELS[predicted_class],
                'confidence': confidence,
                'probabilities': {
                    self.EMOTION_LABELS[i]: prob.item() 
                    for i, prob in enumerate(probabilities[0])
                },
                'model_info': {
                    'has_pretrained': config.has_pretrained_model(),
                    'device': str(self.device)
                }
                # ç§»é™¤featureså­—æ®µé¿å…åºåˆ—åŒ–é—®é¢˜
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                'predicted_class': 4,
                'predicted_label': self.EMOTION_LABELS[4],
                'confidence': 0.5,
                'probabilities': {label: 1.0/7 for label in self.EMOTION_LABELS.values()},
                'model_info': {
                    'has_pretrained': config.has_pretrained_model(),
                    'device': str(self.device)
                },
                'error': str(e)
            }
    
    def predict_batch(self, batch_frames: List[List[np.ndarray]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            batch_frames: æ‰¹é‡è§†é¢‘å¸§åˆ—è¡¨
            
        Returns:
            batch_results: æ‰¹é‡é¢„æµ‹ç»“æœ
        """
        return [self.predict(frames) for frames in batch_frames]
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        model_config_info = config.get_model_info()
        
        return {
            'model_name': 'M3D-FER',
            'num_classes': self.model_args.num_classes,
            'emotion_labels': self.EMOTION_LABELS,
            'device': str(self.device),
            'input_shape': [1, 16, 3, 112, 112],
            'pretrained_info': model_config_info
        } 